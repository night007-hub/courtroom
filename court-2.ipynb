{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvXdh4tfT12A",
        "outputId": "f5c3d9e2-5f36-4b8c-d2d6-71384d9f178e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0\n",
        "!pip install huggingface_hub\n",
        "!pip install groq\n",
        "!pip instrall requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on0LT5JUUerz",
        "outputId": "4d10655b-d2a8-4289-eb16-0ff8d74b57a0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28.0 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28.0) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (1.19.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "ERROR: unknown command \"instrall\" - maybe you meant \"install\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "from huggingface_hub import InferenceClient\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "frULk7TKUoV1"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "case_row = data.iloc[497]\n",
        "full_text = case_row['text']\n",
        "case_summary = full_text\n",
        "accusation = full_text\n",
        "testimony_prompt = full_text"
      ],
      "metadata": {
        "id": "QVVZN8PAU0tX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0w0-XWWpKhyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.colab\n",
        "from huggingface_hub import InferenceClient, login, HfFolder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "\n",
        "if hf_token and not HfFolder.get_token():\n",
        "    login(token=hf_token)\n",
        "\n",
        "\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\", token=hf_token) #tokenizer\n",
        "\n",
        "#truncate the prompt\n",
        "def truncate_prompt(prompt, max_input_tokens=3900):\n",
        "    input_ids = hf_tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
        "    if len(input_ids) > max_input_tokens:\n",
        "        prompt = hf_tokenizer.decode(input_ids[:max_input_tokens], skip_special_tokens=True)\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "wj8MW4HNOED3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import google.colab\n",
        "class Agent:\n",
        "    def __init__(self, name, role, llm_type):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.llm_type = llm_type.lower()\n",
        "        self.memory = [] #memory for each agent is stored\n",
        "\n",
        "        #initialization of hugging face client\n",
        "        if self.llm_type == \"hf\":\n",
        "            self.hf_client = InferenceClient(\"google/flan-t5-base\") #google flan or 'microsoft/Phi-3-mini-4k-instruct'\n",
        "\n",
        "        elif self.llm_type == 'groq':\n",
        "          self.groq_api_key = os.getenv(\"GROQ_API\")\n",
        "          self.groq_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "\n",
        "    def add_to_memory(self, prompt, response):\n",
        "      self.memory.append({\"prompt\": prompt, \"response\": response})\n",
        "\n",
        "    def get_context(self, window_size=2): #remembers 2 converstaions\n",
        "      if not self.memory:\n",
        "        return \"\"  # is empty if there is no memory yet\n",
        "      context = \"\"\n",
        "      for item in self.memory[-window_size:]:\n",
        "          context += f\"User: {item['prompt']}\\n{self.name}: {item['response']}\\n\"\n",
        "      return context\n",
        "\n",
        "\n",
        "\n",
        "    def generate_response(self, prompt):\n",
        "        context = self.get_context()\n",
        "        full_prompt = context + f\"User: {prompt}\\n{self.name}:\" #total prompt including memory\n",
        "\n",
        "        if self.llm_type == 'hf':\n",
        "          response = self.query_huggingface(full_prompt)\n",
        "\n",
        "        elif self.llm_type == 'groq':\n",
        "          response = self.query_groq(full_prompt)\n",
        "          time.sleep(15)\n",
        "\n",
        "\n",
        "        else:\n",
        "            response = f\"{self.name} ({self.role}): Unsupported LLM type.\"\n",
        "\n",
        "        self.add_to_memory(prompt, response)\n",
        "        return response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def query_huggingface(self, prompt):\n",
        "      try:\n",
        "\n",
        "        prompt = truncate_prompt(prompt, max_input_tokens=2048) #truncate the prompt\n",
        "        return self.hf_client.text_generation(prompt, max_new_tokens=300)\n",
        "      except Exception as e:\n",
        "\n",
        "        #if failed,try with a smaller truncated prompt\n",
        "        try:\n",
        "            return \"[HuggingFace Error - retrying with shorter prompt] \" + \\\n",
        "                   self.hf_client.text_generation(prompt[:300], max_new_tokens=30)\n",
        "        except Exception as e2:\n",
        "            return f\"[HF Final Error] {str(e2)}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def query_groq(self, prompt):\n",
        "      try:\n",
        "\n",
        "          if len(prompt) > 7000 :\n",
        "            prompt = prompt[-7000:] #take 7000\n",
        "          headers = {\n",
        "            'Authorization': f'Bearer {self.groq_api_key}',\n",
        "            'Content-Type': 'application/json'\n",
        "          }\n",
        "          messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are {self.name}, a {self.role} in a courtroom. Respond accordingly.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "          ]\n",
        "          payload = {\n",
        "              \"model\": \"llama-3.3-70b-versatile\",\n",
        "              \"messages\": messages,\n",
        "              \"max_tokens\": 300,\n",
        "              \"temperature\": 0.3,\n",
        "              \"top_p\" : 0.8,\n",
        "              \"stream\" : False\n",
        "          }\n",
        "          response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=payload)\n",
        "          response_json = response.json()\n",
        "          if response.status_code == 200:\n",
        "            return response_json['choices'][0]['message']['content'].strip()\n",
        "\n",
        "          elif response.status_code == 429:  # rate limit\n",
        "\n",
        "            if \"Retry-After\" in response.headers:\n",
        "                wait_time = float(response.headers[\"Retry-After\"])\n",
        "            else:\n",
        "                wait_time = 6  # fallback\n",
        "            print(f\"[Rate Limit] Waiting {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "            return self.query_groq(prompt)\n",
        "\n",
        "          else:\n",
        "            return f\"[Groq Error] {response_json.get('error', 'Unknown error')}\"\n",
        "      except Exception as e:\n",
        "          return f\"[Groq Error] {str(e)}\"\n"
      ],
      "metadata": {
        "id": "BBu-LzjTXoRg"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CourtroomAgents:\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "\n",
        "    def add_agent(self, name, role, llm_type):\n",
        "        agent = Agent(name, role, llm_type)\n",
        "        self.agents[role.lower()] = agent\n",
        "\n",
        "    def get_agent(self, role):\n",
        "        return self.agents.get(role.lower(), None)\n",
        "\n",
        "    def simulate_opening_statements(self, case_summary, accusation):\n",
        "        print(\" Trial Opening Statements\")\n",
        "\n",
        "        judge = self.get_agent('judge')\n",
        "        prosecutor = self.get_agent('prosecutor')\n",
        "        defense = self.get_agent('defense lawyer')\n",
        "        witness = self.get_agent('witness')\n",
        "        defendent = self.get_agent('defendent')\n",
        "        plaintiff = self.get_agent('plaintiff')\n",
        "        # Judge opens court\n",
        "        judge_prompt = f\"You are a judge. Open the courtroom and summarize the case precisely without showing any bias: {case_summary}\"\n",
        "        casetype =judge.generate_response(f\"you have read the summary of the case : {case_summary} now tell what type of case it is(answer should be of the form (civil case) or (criminal case) in one word)\")\n",
        "        print(f\"============== {casetype} ====================\")\n",
        "        print(f\" Judge {judge.name}:\")\n",
        "        print(judge.generate_response(judge_prompt))\n",
        "\n",
        "        # Prosecutor gives opening statement\n",
        "        prosecutor_prompt = f\"You are a prosecutor. Present the accusations against the defendant correctly and shortly.Be confident and aggressive : {accusation}\"\n",
        "        print(f\"\\n Prosecutor {prosecutor.name}:\")\n",
        "        print(prosecutor.generate_response(prosecutor_prompt))\n",
        "\n",
        "        #plaintiff gives his side\n",
        "        if \"civil\" in casetype.lower():\n",
        "          plaintiff_prompt = f\"You are the plaintiff. Express your distress to the judge: {case_summary}\"\n",
        "          print(f\"\\n Plaintiff {plaintiff.name}:\")\n",
        "          print(plaintiff.generate_response(plaintiff_prompt))\n",
        "\n",
        "\n",
        "          # Defense gives opening statement\n",
        "          defense_prompt = f\"You are a defense lawyer. Begin your defense for the accused.Be calculative and answer more accurately. Accusation: {accusation}\"\n",
        "          print(f\"\\n Defense Lawyer {defense.name}:\")\n",
        "          print(defense.generate_response(defense_prompt))\n",
        "\n",
        "    def simulate_witness_testimony(self, witness_name, role, testimony_prompt):\n",
        "        witness = self.get_agent(\"witness\")\n",
        "        print(f\"\\n {witness_name} ({role}) is called to testify.\")\n",
        "        testimony = witness.generate_response(f\"based on the following text produce a testimony for the witness:\\n\\n'{testimony_prompt}'\") #asking llm to observe a testimony in the text\n",
        "\n",
        "        response = witness.generate_response(f\"you need to Testify as a {role} on the following: {testimony}\") #witness gives his testification\n",
        "        print(f\"\\n{witness_name} testifies: {response}\")\n",
        "\n",
        "    def simulate_cross_examination(self, questioning_party, questioned_party, testimony):\n",
        "\n",
        "      questioning_agent = self.get_agent(questioning_party)\n",
        "      questioned_agent = self.get_agent(questioned_party)\n",
        "\n",
        "      # Ask LLM to generate a cross-exam question based on the testimony\n",
        "      question_prompt = f\"Based on the following testimony, generate a critical cross-examination question:\\n\\n'{testimony}'\"\n",
        "      question = questioning_agent.generate_response(question_prompt)\n",
        "\n",
        "      print(f\"\\n Cross-Examination by {questioning_party} of {questioned_party}\")\n",
        "      print(f\"{questioning_party}: {question}\")\n",
        "\n",
        "      # questioned party responds to the question\n",
        "      response = questioned_agent.generate_response(f\"{questioning_party} asks: {question} generate a precise response\")\n",
        "      print(f\"{questioned_party}: {response}\")\n",
        "\n",
        "    def simulate_closing_statements(self):\n",
        "        jury = self.get_agent('jury')\n",
        "        print(\"\\n Closing Statements\")\n",
        "\n",
        "        judge = self.get_agent('judge')\n",
        "        prosecutor = self.get_agent('prosecutor')\n",
        "        defense = self.get_agent('defense lawyer')\n",
        "\n",
        "        # Prosecutor closing statement\n",
        "        prosecutor_prompt = \"Make your closing argument and request the judge to take appropriate measures.\"\n",
        "        print(f\"\\n Prosecutor {prosecutor.name}:\")\n",
        "        print(prosecutor.generate_response(prosecutor_prompt))\n",
        "\n",
        "        # Defense closing statement\n",
        "        defense_prompt = \"Make your closing argument and request the judge to take appropriate measures.\"\n",
        "        print(f\"\\n Defense Lawyer {defense.name}:\")\n",
        "        print(defense.generate_response(defense_prompt))\n",
        "\n",
        "        # Judge gives the verdict\n",
        "        judge_prompt = \"Give the verdict on the case considering each and every detail.\"\n",
        "        print(f\"\\n Judge {judge.name}:\")\n",
        "        print(judge.generate_response(judge_prompt))\n",
        "\n",
        "        #jury gives the verdict\n",
        "        jury_prompt = \"learn about the whole case and inspect all details and tell whether the justice is done or not\"\n",
        "        print(f\"\\n Jury gives the statement: \\n\")\n",
        "        print(jury.generate_response(jury_prompt))\n",
        ""
      ],
      "metadata": {
        "id": "wVHt-dCPY-Ss"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "court = CourtroomAgents()\n",
        "\n",
        "#creating agents\n",
        "court.add_agent(\"Justice Ray\", \"Judge\", \"groq\")\n",
        "court.add_agent(\"Ava\", \"Prosecutor\", \"groq\")\n",
        "court.add_agent(\"Leo\", \"Defense Lawyer\", \"groq\")\n",
        "court.add_agent(\"Bob\", \"witness\", \"groq\")\n",
        "court.add_agent(\"defendent\",\"defendent\",\"hf\")\n",
        "court.add_agent(\"jury\",\"jury\",\"groq\")\n",
        "court.add_agent(\"plaintiff\",\"plaintiff\",\"groq\")\n",
        "\n",
        "#run the process\n",
        "court.simulate_opening_statements(case_summary, accusation)\n",
        "court.simulate_witness_testimony(\"witness\",\"witness\",testimony_prompt)\n",
        "court.simulate_cross_examination(\"Defense Lawyer\",\"Prosecutor\",testimony_prompt)\n",
        "court.simulate_cross_examination(\"Prosecutor\",\"witness\",testimony_prompt)\n",
        "court.simulate_cross_examination(\"Prosecutor\",\"Defense Lawyer\",testimony_prompt)\n",
        "court.simulate_cross_examination(\"Prosecutor\",\"Defendent\",accusation)\n",
        "court.simulate_closing_statements()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDqMT90bZWTo",
        "outputId": "c0b59bc0-6006-4fd7-af51-8a91532519ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Trial Opening Statements\n",
            "[Rate Limit] Waiting 744.0 seconds...\n"
          ]
        }
      ]
    }
  ]
}